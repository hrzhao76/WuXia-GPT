{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "from pathlib import Path\n",
    "import jieba\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = (Path().resolve() / '../data/txt/修订版天龙八部.txt').resolve()\n",
    "output_dir_path = (Path().resolve() / '../data/TianLong_jieba').resolve()\n",
    "assert input_file_path.exists()\n",
    "if not output_dir_path.exists():\n",
    "    output_dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,258,853\n",
      "vocab size: 50,912\n",
      "length of sentences in tokens: 839,971\n"
     ]
    }
   ],
   "source": [
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "seg_list = jieba.lcut(data)\n",
    "tokens = sorted(list(set(seg_list)))\n",
    "vocab_size = len(tokens)\n",
    "# print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "print(f\"length of sentences in tokens: {len(seg_list):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(tokens) }\n",
    "itos = { i:ch for i,ch in enumerate(tokens) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 755,973 tokens\n",
      "val has 83,998 tokens\n"
     ]
    }
   ],
   "source": [
    "# create the train and test splits\n",
    "n = len(seg_list)\n",
    "train_data = seg_list[:int(n*0.9)]\n",
    "val_data = seg_list[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(output_dir_path / 'train.bin')\n",
    "val_ids.tofile(output_dir_path / 'val.bin')\n",
    "\n",
    "# save the meta information as well, to help us encode/decode later\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(output_dir_path / 'meta.pkl', 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token '神农帮' not in vocab\n",
      "token '动了手' not in vocab\n",
      "token '凌霄子' not in vocab\n",
      "token '是啊' not in vocab\n",
      "token '要道' not in vocab\n",
      "token '说是' not in vocab\n",
      "token '谁也' not in vocab\n",
      "token '咬着' not in vocab\n",
      "token '掷去' not in vocab\n",
      "token '他的' not in vocab\n",
      "token '笑道' not in vocab\n",
      "token '吃不吃' not in vocab\n"
     ]
    }
   ],
   "source": [
    "# tokens from ChatGPT\n",
    "test_tokens = \\\n",
    "[\n",
    "    \"左子穆\", \"道\", \"是\", \"跟\", \"神农帮\", \"动了手\", \"么\", \n",
    "    \"凌霄子\", \"道\", \"是啊\", \"他们\", \"把守\", \"了\", \n",
    "    \"各处\", \"要道\", \"说是\", \"不到\", \"明日\", \"天亮\", \n",
    "    \"谁也\", \"不许\", \"下山\", \"梁上\", \"那\", \"少女\", \n",
    "    \"口里\", \"咬着\", \"瓜子\", \"两只\", \"脚\", \"一荡\", \n",
    "    \"一荡\", \"的\", \"忽然\", \"将\", \"一粒\", \"瓜子\", \n",
    "    \"往\", \"段誉\", \"头上\", \"掷去\", \"正中\", \"他的\", \n",
    "    \"额头\", \"笑道\", \"喂\", \"你\", \"吃不吃\", \"瓜子\"\n",
    "]\n",
    "\n",
    "for token in test_tokens:\n",
    "    if token not in stoi:\n",
    "        print(f\"token '{token}' not in vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
